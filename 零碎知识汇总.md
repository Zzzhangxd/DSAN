1. 3x3卷积的作用：  
两个3x3卷积核和一个5x5的卷积核得到的感受野相同，三个3x3和7x7相同。  
但是三个3x3卷积核的参数个数比一个7x7卷积核参数个数要少。  
减少了模型参数，加快训练速度。  
同时，3x3卷积非线性程度更高，可以表示更复杂的函数。小的卷积核可以提取细小的特征，由小到大比较抽象的特在。  
但层数的加深会产生一串连锁效应，可能会效果提升，但也可能变差，例如梯度消失。

2. 训练网络时会出现loss增大的情况  
学习率过高，如果学习率设置过高，可能会导致损失函数在优化过程中跳过优异点，从而造成损失函数不断振荡。

3. 加深网络为什么错误率上升  
训练过程中网络的正，反向信息流动不顺畅，网络没有被充分训练。  

4. ResNet的主要贡献：  
提出了一种残差模块，通过堆叠残差模块可以构建任意深度的神经网络，而不会出现"退化"现象。跳跃连接：最差情况是F(x)什么都不学习，21层啥也不干，那么22层和20层性能一样。跳跃连接使得梯度反向传播的梯度至少为1  
提出了批归一化方法来对抗梯度消失，该方法降低了网络训练过程对于权重初始化的依赖  
提出了一种针对ReLU激活i函数的初始化方法——He， Kaiming初始化方法  
瓶颈结构：1x1->3x3->1x1  
N = （W - F + 2P） / S向下取整后+1

5. 全局平均池化代替全连接层  
全连接层起到分类器的作用。  
卷积层，池化层和激活函数层等操作将原始数据映射到隐层特征空间，全连接层则是将学到的分布式特在表示映射到样本标记空间。  
极大地减少了网络的参数量，相当于在网络结构上做正则，防止模型发生过拟合

6. 为什么使用全局平均池化代替全连接层后，网络的收敛速度会变慢  
CNN+GAP的结构，使用GAP代替FC，模型参数骤减，此时模型的学习压力全部前导到CNN层，相比于CNN+FC，此时CNN层不仅仅要学习低层的通用特征，还要学习到更加高级的分类特征，学习难度变大，网络收敛速度变慢

7. 残差网络性能好，残差网络可以看作一种集成模型